{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas\n",
    "import numpy\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools import categorical, add_constant\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "import scipy.stats as sps\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152277</th>\n",
       "      <td>2014-06-01T06:37:09.000Z</td>\n",
       "      <td>A few improvements prior to #bristolvolksfest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152278</th>\n",
       "      <td>2014-06-01T06:31:47.000Z</td>\n",
       "      <td>VW to accelerate US model rollout - Filed unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152279</th>\n",
       "      <td>2014-06-01T03:19:06.000Z</td>\n",
       "      <td>#Volkswagen Group to invest 100 million euros ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152280</th>\n",
       "      <td>2014-06-01T01:27:42.000Z</td>\n",
       "      <td>VW to accelerate US model rollout: Filed under...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152281</th>\n",
       "      <td>2014-06-01T01:08:41.000Z</td>\n",
       "      <td>The end. Once more time thank you so much @veu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      created_at  \\\n",
       "152277  2014-06-01T06:37:09.000Z   \n",
       "152278  2014-06-01T06:31:47.000Z   \n",
       "152279  2014-06-01T03:19:06.000Z   \n",
       "152280  2014-06-01T01:27:42.000Z   \n",
       "152281  2014-06-01T01:08:41.000Z   \n",
       "\n",
       "                                                     text  \n",
       "152277  A few improvements prior to #bristolvolksfest ...  \n",
       "152278  VW to accelerate US model rollout - Filed unde...  \n",
       "152279  #Volkswagen Group to invest 100 million euros ...  \n",
       "152280  VW to accelerate US model rollout: Filed under...  \n",
       "152281  The end. Once more time thank you so much @veu...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pandas.read_csv('pydata_vw_tweets.csv').tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_tweets_df(starting_date, final_date, time_window):\n",
    "    \"\"\"\n",
    "    Create pandas DataFrame with tweets ranging between two dates and resample\n",
    "    it to represent intervals of fixed duration.\n",
    "    \n",
    "    :param time_window: str e.g. '60Min', or '10Min'\n",
    "    \"\"\"\n",
    "\n",
    "    # csv has timestamps in first column named 'created_at'\n",
    "    tweets_df = pandas.DataFrame(pandas.read_csv(\n",
    "        'pydata_vw_tweets.csv', parse_dates=[0]))\n",
    "    tweets_df = tweets_df.created_at\n",
    "    tweets_df = pandas.DataFrame(tweets_df)\n",
    "\n",
    "    tweets_df.loc[:, 'n'] = 1\n",
    "    \n",
    "    tweets_df.set_index(['created_at'], inplace=True)\n",
    "\n",
    "    # dummy tweet is used to resample until the very end\n",
    "    dummy_tweet = {'created_at': final_date,\n",
    "                   'n': 1}\n",
    "    dummy_tweets_df = pandas.DataFrame([dummy_tweet]).set_index('created_at')\n",
    "\n",
    "    aux_df = pandas.concat([dummy_tweets_df, tweets_df])\n",
    "    \n",
    "    # resample dataframe and count number of tweets for every time_window (0 if no tweet) \n",
    "    df_reshaped = aux_df.resample(time_window, label='left', closed='left').sum().fillna(0)\n",
    "\n",
    "    df_reshaped.loc[:, 'weekday'] = df_reshaped.index.map(lambda x: x.weekday)\n",
    "    df_reshaped.loc[:, 'hour'] = df_reshaped.index.map(lambda x: x.hour)\n",
    "    # shift by 1 since it's n for previous period\n",
    "    df_reshaped.loc[:, 'n_prev_period'] = df_reshaped.n.shift(1).fillna(0)\n",
    "    df_reshaped.loc[:, 'n_prev_2_period'] = df_reshaped.n_prev_period.shift(1).fillna(0)\n",
    "\n",
    "    df_reshaped = df_reshaped.ix[starting_date:final_date].ix[:-1]\n",
    "    return df_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        n  weekday  hour  n_prev_period  n_prev_2_period\n",
      "created_at                                                              \n",
      "2014-06-03 16:00:00  32.0        1    16           15.0              5.0\n",
      "2014-06-03 17:00:00   7.0        1    17           32.0             15.0\n",
      "2014-06-03 18:00:00   2.0        1    18            7.0             32.0\n",
      "2014-06-03 19:00:00   3.0        1    19            2.0              7.0\n",
      "2014-06-03 20:00:00   5.0        1    20            3.0              2.0\n",
      "                        n  weekday  hour  n_prev_period  n_prev_2_period\n",
      "created_at                                                              \n",
      "2015-10-03 19:00:00  10.0        5    19           37.0              6.0\n",
      "2015-10-03 20:00:00  18.0        5    20           10.0             37.0\n",
      "2015-10-03 21:00:00  26.0        5    21           18.0             10.0\n",
      "2015-10-03 22:00:00   9.0        5    22           26.0             18.0\n",
      "2015-10-03 23:00:00   3.0        5    23            9.0             26.0\n"
     ]
    }
   ],
   "source": [
    "# load some data between two specified dates\n",
    "data = create_tweets_df(datetime(2014, 6, 3, 16), datetime(2015, 10, 4),  '60Min')\n",
    "print data.head()\n",
    "print data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(training_data):\n",
    "    \"\"\"\n",
    "    Create a Poisson model given training data in a pandas DataFrame form\n",
    "\n",
    "    :param training_data: pandas.DataFrame with specific columns for predictors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    model_family = sm.families.Poisson()\n",
    "    \n",
    "    # use categorical variable, which means that e.g. to represent day 4, the array is [0 0 0 0 1 0 0]\n",
    "    dummy_hour = categorical(numpy.asarray(training_data['hour']))[:, 1:]\n",
    "    dummy_weekday = categorical(numpy.asarray(training_data['weekday']))[:, 1:]\n",
    "\n",
    "    # target variable\n",
    "    endog = numpy.asmatrix(training_data['n']).T\n",
    "\n",
    "    # combine predictors\n",
    "    exog = numpy.concatenate((training_data[['n_prev_period', 'n_prev_2_period']],\n",
    "                           dummy_hour, dummy_weekday), axis=1)\n",
    "    exog = add_constant(exog, prepend=False)\n",
    "\n",
    "    model = GLM(endog, exog, family=model_family)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_and_fit_model(training_data):\n",
    "    \"\"\"\n",
    "    Create and fit GLM model on training data\n",
    "    \n",
    "    :param: training_data: data to use when training the model\n",
    "    :return: fitted model\n",
    "    \"\"\"\n",
    "    \n",
    "    glm_model = create_model(training_data)\n",
    "\n",
    "    # fit the model\n",
    "    model_results = glm_model.fit(maxiter=100)\n",
    "    print \"Model generated.\"\n",
    "\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_design_matrix(n_minus_1, n_minus_2, hour, weekday):\n",
    "    \"\"\"\n",
    "    Create the design matrix for predictors for a single datapoint,\n",
    "    each column correspond to a predictor.\n",
    "    NB: hour and day are set as indicators,\n",
    "        i.e. arrays with 0s and one element set to 1\n",
    "\n",
    "    :param n_minus_1: number of tweets in the previous period\n",
    "    :param n_minus_2: number of tweets seen 2 periods earlier\n",
    "    :param hour: hour of the day\n",
    "    :param weekday: index for the day of the week, Monday is 0\n",
    "    :return: matrix with predictors\n",
    "    \"\"\"\n",
    "    hour_indicator = [0] * 24\n",
    "    hour_indicator[int(hour)] = 1\n",
    "\n",
    "    day_indicator = [0] * 7\n",
    "    day_indicator[int(weekday)] = 1\n",
    "    \n",
    "    # add constant which refer to the intercept \n",
    "    constant = 1\n",
    "    return numpy.concatenate(([n_minus_1], [n_minus_2], hour_indicator,\n",
    "                           day_indicator, [constant]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  8,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of one single datapoint in the feature space\n",
    "display(create_design_matrix(10, 8, 6, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model_results, design_matrix, n_events, alpha):\n",
    "    \"\"\"\n",
    "    Detect if the volume is anomalous using a given model and threshold.\n",
    "\n",
    "    :param model_results: fitted model\n",
    "    :param design_matrix: datapoint in the predictors space\n",
    "    :param n_events: volume of tweet in the current window - check if this is anomalous, given the model\n",
    "    :param alpha: threshold \n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    prediction = model_results.model.predict(\n",
    "        model_results.params, design_matrix)\n",
    "    # compute the number of events which correspond to probability threshold alpha. \n",
    "    # This represents the upper limit for the numeber of tweets created in the current period\n",
    "    upper_limit = max(sps.poisson.ppf(1 - alpha, prediction), 0)\n",
    "    is_anomaly = n_events > upper_limit\n",
    "    \n",
    "    # Similar to computing the probability of seeing more than n events \n",
    "    # and comparing directly with alpha.\n",
    "    # Only difference is that upper_limit is a discrete value\n",
    "    \n",
    "    #     p_value = (1 - sps.poisson.cdf(k=n_events, mu=prediction))\n",
    "    #    is_anomaly = p_value < alpha\n",
    "    \n",
    "    return bool(is_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect(n_tweets, predictors, threshold_alpha, training_data):\n",
    "    \"\"\"\n",
    "    Detect if the number of tweets is anomalous, given the predictors and a\n",
    "    lower tail threshold\n",
    "\n",
    "    :param n_tweets: number of tweets seen in the current period\n",
    "    :param predictors: dictionary with predictors\n",
    "    :param threshold_alpha: threshold for the pvalue\n",
    "    :param training_data: dataframe with training data\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    glm_model = generate_and_fit_model(training_data)\n",
    "\n",
    "    n_minus_1 = predictors['n_prev_period']\n",
    "    n_minus_2 = predictors['n_prev_2_period']\n",
    "    weekday = predictors['weekday']\n",
    "    hour = predictors['hour']\n",
    "\n",
    "    design_matrix = create_design_matrix(n_minus_1, n_minus_2, hour, weekday)\n",
    "    return predict(glm_model, design_matrix, n_tweets, threshold_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Single datapoint detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model generated.\n",
      "Anomalous? True\n"
     ]
    }
   ],
   "source": [
    "predictors = {'n_prev_period': 3, \n",
    "              'n_prev_2_period': 5, \n",
    "              'hour': 16, \n",
    "              'weekday': 4}\n",
    "\n",
    "example_data = create_tweets_df(datetime(2014, 5, 1), datetime(2016, 5, 5), time_window='10Min')\n",
    "is_anomaly_example = detect(n_tweets=19, predictors=predictors, \n",
    "                            threshold_alpha=1e-4, training_data=example_data)\n",
    "print \"Anomalous? {0}\".format(is_anomaly_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "* applicable to any data about events happening in time\n",
    "* quick to train and fast in the detection\n",
    "* can be used with real time data to detect/predict the next datapoint coming in (no need to wait for more data)\n",
    "* easily extendible to include more and different predictors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
